{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "TAGS_FILE_NAME = 'desktop/data/top10_tags.tsv'\n",
    "DS_FILE_NAME = 'desktop/data/stackoverflow_sample_125k.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "\n",
    "        self._vocab = {}\n",
    "\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "\n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    sigma = 1 / (1 + np.exp(-z)) if z >= 0 else 1 - 1 / (1 + np.exp(z))\n",
    "                    \n",
    "                    sample_loss += -y * np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y) * np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: 19.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     accuracy_level=0.9):\n",
    "\n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy = []\n",
    "        with open(fname, 'r') as f:            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "                predicted_tags = None\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    sample_loss += -y*np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    else:\n",
    "                        if predicted_tags is None:\n",
    "                            predicted_tags = []\n",
    "                        if sigma > accuracy_level:\n",
    "                            predicted_tags.append(tag)\n",
    "                    \n",
    "                n += 1\n",
    "                                        \n",
    "                self._loss.append(sample_loss)\n",
    "                if predicted_tags is not None:\n",
    "                    accuracy.append(len(tags.intersection(predicted_tags))/len(tags.union(predicted_tags)))\n",
    "            \n",
    "        return(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     accuracy_level=0.9,\n",
    "                     lmbda=0.01):\n",
    "\n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy = []\n",
    "        with open(fname, 'r') as f:            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "                predicted_tags = None\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    sample_loss += -y*np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "                        \n",
    "                        r_buf = {}\n",
    "                        for word in sentence:\n",
    "                            if word not in r_buf:\n",
    "                                r = learning_rate*lmbda*self._w[tag][self._vocab[word]]\n",
    "                                r_buf[word] = True\n",
    "                            else:\n",
    "                                r = 0\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw + r\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    else:\n",
    "                        if predicted_tags is None:\n",
    "                            predicted_tags = []\n",
    "                        if sigma > accuracy_level:\n",
    "                            predicted_tags.append(tag)\n",
    "                    \n",
    "                n += 1\n",
    "                                        \n",
    "                self._loss.append(sample_loss)\n",
    "                if predicted_tags is not None:\n",
    "                    accuracy.append(len(tags.intersection(predicted_tags))/len(tags.union(predicted_tags)))\n",
    "            \n",
    "        return(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     accuracy_level=0.9,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1):\n",
    "\n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy = []\n",
    "        with open(fname, 'r') as f:            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "                predicted_tags = None\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    sample_loss += -y * np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y) * np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        r_buf = {}\n",
    "                        for word in sentence:\n",
    "                            if word not in r_buf:\n",
    "                                r = 2 * learning_rate * lmbda * gamma * self._w[tag][self._vocab[word]] + \\\n",
    "                                    learning_rate * lmbda*(1 - gamma) * np.sign(self._w[tag][self._vocab[word]])\n",
    "                                r_buf[word] = True\n",
    "                            else:\n",
    "                                r = 0\n",
    "                                \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw + r\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    else:\n",
    "                        if predicted_tags is None:\n",
    "                            predicted_tags = []\n",
    "                        if sigma > accuracy_level:\n",
    "                            predicted_tags.append(tag)\n",
    "                    \n",
    "                n += 1\n",
    "                                        \n",
    "                self._loss.append(sample_loss)\n",
    "                if predicted_tags is not None:\n",
    "                    accuracy.append(len(tags.intersection(predicted_tags))/len(tags.union(predicted_tags)))\n",
    "                    \n",
    "                return(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: 0.59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8: с#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._tags = set(tags)\n",
    "        self._word_stats = defaultdict(int)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     accuracy_level=0.9,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1,\n",
    "                     update_vocab=True):\n",
    "\n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy = []\n",
    "        with open(fname, 'r') as f:            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "                predicted_tags = None\n",
    "                \n",
    "                for ix_tag, tag in enumerate(self._tags):\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab and update_vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        if word not in self._vocab:\n",
    "                            continue\n",
    "                        if update_vocab and ix_tag == 0 and n < top_n_train:\n",
    "                            self._word_stats[self._vocab[word]] += 1\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    sample_loss += -y*np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        for word in sentence:  \n",
    "                            if word not in self._vocab:\n",
    "                                continue\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw \\\n",
    "                                + 2 * learning_rate * lmbda * gamma * self._w[tag][self._vocab[word]] \\\n",
    "                                + learning_rate * lmbda *(1 - gamma) * np.sign(self._w[tag][self._vocab[word]])\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    else:\n",
    "                        if predicted_tags is None:\n",
    "                            predicted_tags = []\n",
    "                        if sigma > accuracy_level:\n",
    "                            predicted_tags.append(tag)\n",
    "                    \n",
    "                n += 1\n",
    "                                        \n",
    "                self._loss.append(sample_loss)\n",
    "                if predicted_tags is not None:\n",
    "                    accuracy.append(len(tags.intersection(predicted_tags))/len(tags.union(predicted_tags)))\n",
    "            \n",
    "        return(np.mean(accuracy))\n",
    "    \n",
    "    def filter_vocab(self, n=10000):\n",
    "        keep_words = set([wid for (wid, wn) in sorted(self._word_stats.items(), \n",
    "                                                      key=lambda t: t[1], reverse=True)[:n]])\n",
    "        self._vocab = dict([(k, v) for (k, v) in self._vocab.items() if v in keep_words])\n",
    "        for tag in self._tags:\n",
    "            self._w[tag] = dict([(k, v) for (k, v) in self._w[tag].items() if k in keep_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9: 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10: ios, php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
